{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# install requirements"
      ],
      "metadata": {
        "id": "1wyGFD5VXC6P"
      },
      "id": "1wyGFD5VXC6P"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U openai-whisper\n",
        "!pip install --upgrade datasets[audio] transformers accelerate evaluate jiwer tensorboard gradio\n",
        "!pip install torchaudio\n",
        "!pip install noisereduce librosa soundfile\n",
        "!pip install pyannote.audio\n"
      ],
      "metadata": {
        "id": "97E8PJuqXQRu"
      },
      "id": "97E8PJuqXQRu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import {\n",
        "      AutoTokenizer,\n",
        "      AutoModelForCausalLM,\n",
        "      pipeline,\n",
        "      AutoModelForSpeechSeq2Seq,\n",
        "      AutoProcessor,\n",
        "      Wav2Vec2ForCTC,\n",
        "      Wav2Vec2Processor,\n",
        "    }\n",
        "from datasets import {\n",
        "      load_dataset,\n",
        "      Dataset,\n",
        "      load_metric\n",
        "    }\n",
        "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
        "from IPython.display import Audio, ipd\n",
        "from collections import defaultdict\n",
        "from pyannote.core import Segment\n",
        "import noisereduce as nr\n",
        "import soundfile as sf\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torchaudio\n",
        "import librosa\n",
        "import torch\n",
        "import time\n"
      ],
      "metadata": {
        "id": "8a_CfDlKu-9D"
      },
      "id": "8a_CfDlKu-9D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(\"Your HuggingFace Token\")"
      ],
      "metadata": {
        "id": "0xVcw-1oXQOp"
      },
      "id": "0xVcw-1oXQOp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reduce Noise on Audio"
      ],
      "metadata": {
        "id": "u9y8KiusYk-i"
      },
      "id": "u9y8KiusYk-i"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "file_name = \"test.wav\"\n",
        "\n",
        "data, srate = librosa.load(file_name, sr=None)\n",
        "\n",
        "reduced_noise = nr.reduce_noise(y=data, sr=srate)\n",
        "\n",
        "sf.write(\"denoised.wav\", data=reduced_noise, samplerate=srate)"
      ],
      "metadata": {
        "id": "jaAVNV5TYttr"
      },
      "id": "jaAVNV5TYttr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Speaker Diarization"
      ],
      "metadata": {
        "id": "sF010qt0Xazx"
      },
      "id": "sF010qt0Xazx"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyannote.audio import Pipeline\n",
        "\n",
        "pipeline = Pipeline.from_pretrained(\n",
        "  \"pyannote/speaker-diarization-3.1\",\n",
        "  use_auth_token=\"Your HuggingFace Token\")\n",
        "\n",
        "pipeline.to(torch.device(\"cuda\"))\n",
        "\n",
        "# run the pipeline on an audio file\n",
        "audio = Audio()\n",
        "waveform, sample_rate = audio(file_name)\n",
        "\n",
        "from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
        "with ProgressHook() as hook:\n",
        "    diarization = pipeline({\"waveform\": waveform, \"sample_rate\": sample_rate}, hook=hook)\n",
        "\n",
        "# dump the diarization output to disk using RTTM format\n",
        "with open(\"audio.rttm\", \"w\") as rttm:\n",
        "    diarization.write_rttm(rttm)"
      ],
      "metadata": {
        "id": "HH3pkPVQXQGY"
      },
      "id": "HH3pkPVQXQGY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_seconds(seconds):\n",
        "    minutes = int(seconds) // 60\n",
        "    secs = int(seconds) % 60\n",
        "    return f\"{minutes}:{secs:02d}\"\n",
        "\n",
        "# print the result\n",
        "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
        "    print(f\"start={format_seconds(turn.start)}s stop={format_seconds(turn.end)}s speaker_{speaker}\")\n"
      ],
      "metadata": {
        "id": "MdQyh-RzXQB7"
      },
      "id": "MdQyh-RzXQB7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Collect segments per speaker\n",
        "speaker_segments = defaultdict(list)\n",
        "for segment, _, speaker in diarization.itertracks(yield_label=True):\n",
        "    start_sample = int(segment.start * sample_rate)\n",
        "    end_sample = int(segment.end * sample_rate)\n",
        "    speaker_segments[speaker].append(waveform[start_sample:end_sample])\n",
        "\n",
        "# Save each speaker's segments into separate files\n",
        "for speaker, segments in speaker_segments.items():\n",
        "    full_speech = np.concatenate(segments)\n",
        "    full_speech = full_speech.astype(np.float32)  # ensure valid format\n",
        "    sf.write(f\"{speaker}.wav\", full_speech, sample_rate, format='WAV', subtype='PCM_16')"
      ],
      "metadata": {
        "id": "Jpf-xlC2YKCl"
      },
      "id": "Jpf-xlC2YKCl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Multiple ASR models"
      ],
      "metadata": {
        "id": "_8TR_rf6aAz6"
      },
      "id": "_8TR_rf6aAz6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. [First ASR Model](https://huggingface.co/m3hrdadfi/wav2vec2-large-xlsr-persian-v3)\n",
        "\n",
        "A Fine-Tuned wav2vec on Persian\n"
      ],
      "metadata": {
        "id": "OoGzrdw7ZPLb"
      },
      "id": "OoGzrdw7ZPLb"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_name_or_path = \"m3hrdadfi/wav2vec2-large-xlsr-persian-v3\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(model_name_or_path, device)\n",
        "\n",
        "processor = Wav2Vec2Processor.from_pretrained(model_name_or_path)\n",
        "model = Wav2Vec2ForCTC.from_pretrained(model_name_or_path).to(device)\n",
        "\n",
        "\n",
        "def speech_file_to_array_fn(batch):\n",
        "    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
        "    speech_array = speech_array.squeeze().numpy()\n",
        "    speech_array = librosa.resample(y=np.asarray(speech_array), orig_sr=sampling_rate, target_sr=processor.feature_extractor.sampling_rate)\n",
        "\n",
        "    batch[\"speech\"] = speech_array\n",
        "    return batch\n",
        "\n",
        "\n",
        "def predict(batch):\n",
        "    features = processor(\n",
        "        batch[\"speech\"],\n",
        "        sampling_rate=processor.feature_extractor.sampling_rate,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    )\n",
        "\n",
        "    input_values = features.input_values.to(device)\n",
        "    attention_mask = features.attention_mask.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_values, attention_mask=attention_mask).logits\n",
        "\n",
        "    pred_ids = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    batch[\"predicted\"] = processor.batch_decode(pred_ids)\n",
        "    return batch\n"
      ],
      "metadata": {
        "id": "G6wmq8z5ZTT0"
      },
      "id": "G6wmq8z5ZTT0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "root_dir = \"./SampleDir/sample.wav\"\n",
        "\n",
        "audio_files = [\n",
        "\t\t\tos.path.join(root_dir, f) for f in os.listdir(root_dir)\n",
        "\t\t\tif f.endswith(\".mp3\")\n",
        "\t\t]\n",
        "\n",
        "dataset = Dataset.from_dict({\"path\": audio_files})\n",
        "\n",
        "dataset = dataset.map(speech_file_to_array_fn)\n",
        "result = dataset.map(predict, batched=True, batch_size=4)\n",
        "asr_output = result['predicted']"
      ],
      "metadata": {
        "id": "60iMISoEZTJW"
      },
      "id": "60iMISoEZTJW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. [Second ASR Model](https://huggingface.co/ghofrani/xls-r-1b-fa-cv8)\n",
        "A Fine-Tuned wav2vec on Persian\n"
      ],
      "metadata": {
        "id": "zv3SVT9FbO6A"
      },
      "id": "zv3SVT9FbO6A"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pipe = pipeline(\"automatic-speech-recognition\", model=\"ghofrani/xls-r-1b-fa-cv8\")"
      ],
      "metadata": {
        "id": "_0CpqjENbYge"
      },
      "id": "_0CpqjENbYge",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "asr_output = pipe('sample.wav')['text']\n",
        "asr_output"
      ],
      "metadata": {
        "id": "qibJYW60bYaZ"
      },
      "id": "qibJYW60bYaZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. [Third ASR Model](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-persian)\n",
        "A Fine-Tuned wav2vec on Persian\n"
      ],
      "metadata": {
        "id": "DBBk_NIBbgfC"
      },
      "id": "DBBk_NIBbgfC"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pipe = pipeline(\"automatic-speech-recognition\", model=\"jonatasgrosman/wav2vec2-large-xlsr-53-persian\")"
      ],
      "metadata": {
        "id": "dIcw7lb9bqmY"
      },
      "id": "dIcw7lb9bqmY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "asr_output = pipe('sample.wav')['text']\n",
        "asr_output"
      ],
      "metadata": {
        "id": "4eB76yXobqfT"
      },
      "id": "4eB76yXobqfT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. [Fourth ASR Model](https://github.com/SYSTRAN/faster-whisper)\n",
        "\n",
        "faster whisper model"
      ],
      "metadata": {
        "id": "L6fUKfI-byoN"
      },
      "id": "L6fUKfI-byoN"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faster-whisper"
      ],
      "metadata": {
        "id": "eCTTdTU4byZ1"
      },
      "id": "eCTTdTU4byZ1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from faster_whisper import WhisperModel\n",
        "\n",
        "model_size = \"turbo\"\n",
        "\n",
        "# Run on GPU with FP16\n",
        "model = WhisperModel(model_size, device=\"cuda\", compute_type=\"float16\")\n",
        "# or run on GPU with INT8\n",
        "# model = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8_float16\")\n",
        "# or run on CPU with INT8\n",
        "# model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\n"
      ],
      "metadata": {
        "id": "cUSQYCxIdMrV"
      },
      "id": "cUSQYCxIdMrV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "st = time.time()\n",
        "# segments, info = model.transcribe(\"test1.ogg\", beam_size=5, language=\"fa\")\n",
        "segments, _ = model.transcribe(\n",
        "    \"sample.wav\",\n",
        "    vad_filter=True,\n",
        "    vad_parameters=dict(min_silence_duration_ms=500),\n",
        ")\n",
        "print(\"es time:\", time.time()-st)\n",
        "print(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
        "text = \"\"\n",
        "for segment in segments:\n",
        "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
        "    text = text + \" \" + segment.text\n",
        "\n",
        "asr_output = text"
      ],
      "metadata": {
        "id": "0epwLVQbdQQW"
      },
      "id": "0epwLVQbdQQW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. [Fifth ASR Model](https://huggingface.co/openai/whisper-large-v3-turbo)\n",
        "\n",
        "Whisper Large v3 Turbo with SDPA Attention"
      ],
      "metadata": {
        "id": "nVWoawGHdoWK"
      },
      "id": "nVWoawGHdoWK"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "model_id = \"openai/whisper-large-v3-turbo\"\n",
        "\n",
        "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True, attn_implementation=\"sdpa\"\n",
        ").to(device)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "model.generation_config.return_timestamps = True\n",
        "model.generation_config.language = 'fa'\n",
        "model.generation_config.task = 'transcribe'\n",
        "model.generation_config.forced_decoder_ids = None\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=model,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    feature_extractor=processor.feature_extractor,\n",
        "    torch_dtype=torch_dtype,\n",
        "    device=device,\n",
        "    batch_size=64,\n",
        "    chunk_length_s=22\n",
        ")"
      ],
      "metadata": {
        "id": "_owmqi-cdv6e"
      },
      "id": "_owmqi-cdv6e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sample, sr = librosa.load(\"sample.mp3\", sr=None)\n",
        "sample = librosa.resample(y=np.asarray(sample), orig_sr=sr, target_sr=processor.feature_extractor.sampling_rate)"
      ],
      "metadata": {
        "id": "XdDBSHcWeYWk"
      },
      "id": "XdDBSHcWeYWk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "start = time.time()\n",
        "# with sdpa_kernel(SDPBackend.MATH):\n",
        "asr_output = pipe(sample)['result']\n",
        "end = time.time()\n",
        "print(\"Estimated time: \", end - start)\n",
        "asr_output"
      ],
      "metadata": {
        "id": "6gh0ES0Vec-Z"
      },
      "id": "6gh0ES0Vec-Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Post-Processing ASR Output using a LLM"
      ],
      "metadata": {
        "id": "y-uDMfgiiNCe"
      },
      "id": "y-uDMfgiiNCe"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def correct_persian_text(misspelled_text):\n",
        "    # Load the model (assumes you have access or downloaded it locally)\n",
        "    corrector = pipeline(\"text-generation\", model=\"google/gemma-3n-e4b-it\", device=0)  # use device=0 for GPU, -1 for CPU\n",
        "\n",
        "    # Prepare the prompt\n",
        "    prompt = f\"\"\"متن زیر شامل برخی کلمات با غلط املایی است. لطفاً نسخهٔ اصلاح‌شدهٔ آن را ارائه بده و تنها متن تصحیح‌شده را بدون هیچ توضیحی برگردان.\n",
        "\n",
        "              متن: «{misspelled_text}»\n",
        "\n",
        "              \"\"\"\n",
        "\n",
        "    # Generate correction\n",
        "    result = corrector(prompt, max_new_tokens=100, do_sample=False)[0][\"generated_text\"]\n",
        "\n",
        "    corrected_part = result.replace(prompt, \"\").strip()\n",
        "    return corrected_part, result\n",
        "\n"
      ],
      "metadata": {
        "id": "E2JnaAOJiaZf"
      },
      "id": "E2JnaAOJiaZf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = asr_output\n",
        "# text = \" سلام و درود من میخوام سیرویس و اینترنت هم از ایدی ایس ایل تبدیل به افتیتیه کنم بعد به مخابرات هم زنگ زدم تماس گرفتم گفتن که قرار شده بررسی کنن و خبر بدن که آیا منطقه ما تحت پوشش هست کنن کود پوستی ها که دادیم به سایت گفت تحت پوشش نیست ولی مخابرات قرار شد خودش از روی آدرس بدیری بکنه و خب هنجا بایمون قبر ندارن اگر که جواب مصبته که من حزینه رو باریس بکنم\"\n",
        "crrt_txt, output = correct_persian_text(text)\n",
        "crrt_txt, output"
      ],
      "metadata": {
        "id": "SlzUIWZYijhG"
      },
      "id": "SlzUIWZYijhG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intent Classification"
      ],
      "metadata": {
        "id": "pFZXk67Wfw_r"
      },
      "id": "pFZXk67Wfw_r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Classify with Bert"
      ],
      "metadata": {
        "id": "ue54E62yf2VK"
      },
      "id": "ue54E62yf2VK"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load the zero-shot classification pipeline with BERT\n",
        "classifier = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=\"bert-base-uncased\"\n",
        ")\n",
        "\n",
        "# 2. Define your text and the categories you want to test\n",
        "sequence_to_classify = \"شرکت اپل از جدیدترین مدل آیفون رونمایی کرد\"\n",
        "candidate_labels = [\"ورزشی\", \"سیاسی\", \"فناوری\", \"علمی\"]\n",
        "\n",
        "# 3. Run the classifier\n",
        "result = classifier(sequence_to_classify, candidate_labels)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "lo6QmKtnf-Vc"
      },
      "id": "lo6QmKtnf-Vc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Classify with a LLM"
      ],
      "metadata": {
        "id": "3hkWjV-ggPYZ"
      },
      "id": "3hkWjV-ggPYZ"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def classify_intent_persian(text, categories, model_id=\"google/gemma-3n-e4b-it\", max_new_tokens=8):\n",
        "    # Load tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "    # Build Persian prompt\n",
        "    category_list = \"\\n- \" + \"\\n- \".join(categories)\n",
        "    prompt = f\"\"\"\n",
        "          شما یک دستیار هوش مصنوعی هستید که نیت جمله‌های کاربران را بر اساس دسته‌بندی‌های زیر مشخص می‌کند:\n",
        "          {category_list}\n",
        "\n",
        "          کاربر: می‌خواستم بدونم چطور می‌تونم اشتراکم رو لغو کنم؟\n",
        "          نیت: درخواست اطلاعات\n",
        "\n",
        "          کاربر: {text}\n",
        "          نیت:\n",
        "          \"\"\"\n",
        "\n",
        "    # Use text-generation pipeline\n",
        "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "    output = pipe(prompt, max_new_tokens=max_new_tokens, do_sample=False, temperature=0)[0][\"generated_text\"]\n",
        "\n",
        "    # Extract the intent after the last \"نیت:\"\n",
        "    intent_line = output.split(\"نیت:\")[-1].strip().split(\"\\n\")[0]\n",
        "    return intent_line\n"
      ],
      "metadata": {
        "id": "MeVCKPUAg1vJ"
      },
      "id": "MeVCKPUAg1vJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categories = [\"درخواست اطلاعات\", \"شکایت\", \"احوال‌پرسی\", \"خداحافظی\"]\n",
        "text = \"سلام، یه سوال داشتم درباره‌ی تمدید اشتراک\"\n",
        "\n",
        "intent = classify_intent_persian(text, categories)\n",
        "print(\"نیت شناسایی‌شده:\", intent)\n"
      ],
      "metadata": {
        "id": "4C5c3cYLhg9_"
      },
      "id": "4C5c3cYLhg9_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text to Speech (TTS)"
      ],
      "metadata": {
        "id": "mJTGd02Xt3IT"
      },
      "id": "mJTGd02Xt3IT"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install edge-tts"
      ],
      "metadata": {
        "id": "BhL4K90st7JC"
      },
      "id": "BhL4K90st7JC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!edge-tts --list-voices"
      ],
      "metadata": {
        "id": "sEfj558xuzgZ"
      },
      "id": "sEfj558xuzgZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!edge-tts --voice  fa-IR-FaridNeural --text \"مشترک گرامی به شماره 245,698 جهت خرید اشتراک باید هزینه ۱۰۰۰ تومان را واریز کنید\" --write-media output.mp3 --write-subtitles hello_in_arabic.srt\n"
      ],
      "metadata": {
        "id": "pWPCkiHhu1mQ"
      },
      "id": "pWPCkiHhu1mQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Audio('output.mp3', autoplay=True)"
      ],
      "metadata": {
        "id": "ZbS6Luhtu25C"
      },
      "id": "ZbS6Luhtu25C",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}